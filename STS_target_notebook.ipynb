{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "import time\n",
    "import csv\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as swords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import twint\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API token information is saved in a separate file (txt|bak) and read in per line\n",
    "TWITTER = open(\"TwiTokens.bak\", \"r\").read().splitlines()\n",
    "auth = tweepy.OAuthHandler(TWITTER[6], TWITTER[7])\n",
    "auth.set_access_token(TWITTER[8], TWITTER[9])\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-headline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = api.get_user('<TARGET_SCREEN_NAME>')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-academy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'ID: {target.id}')\n",
    "print(f'Username: {target.screen_name}')\n",
    "print(f'User Bio: {target.description}')\n",
    "f_date = str(target.created_at)\n",
    "l_date = str(target.status.created_at)\n",
    "print(f'Account birthdate: {f_date}')\n",
    "f_date = f_date.split()\n",
    "f_date = f_date[0]\n",
    "f_date = f_date.split('-')\n",
    "print(f'Last tweet date: {l_date}')\n",
    "l_date = l_date.split()\n",
    "l_date = l_date[0]\n",
    "l_date = l_date.split('-')\n",
    "delta = dt.date(int(l_date[0]), int(l_date[1]), int(l_date[2])) - dt.date(int(f_date[0]), int(f_date[1]), int(f_date[2]))\n",
    "print(f'Active Timeline: {delta}')\n",
    "print(f'Average Tweets Per Day: {target.statuses_count/delta.days}')\n",
    "print(f'Last Tweet: {target.status.text}')\n",
    "print(f'Followers Count: {target.followers_count}')\n",
    "print(f'Friends Count: {target.friends_count}')\n",
    "print(f'Tweets: {target.statuses_count}')\n",
    "follow_ratio = target.friends_count/target.followers_count\n",
    "print(f'Follow Ratio: {follow_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run this for new scrapes. The initial scrape will save output to a textfile. To re-analyze the same target, \n",
    "#skip to the next cell and uncomment the first line\n",
    "target = target.screen_name\n",
    "with open(f'{target}.txt', 'w+') as target_file:\n",
    "            for tweet in tweepy.Cursor(api.user_timeline, id=target).items():\n",
    "                target_file.write(f'{tweet.text}\\n')\n",
    "                target_file.flush()\n",
    "            target_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target = target.screen_name\n",
    "with open(f'{target}.txt', 'r') as target_file:\n",
    "    tweets = target_file.readlines()\n",
    "    word_count = 0\n",
    "    rt_list = []\n",
    "    word_dict = {}\n",
    "    hashtags = {}\n",
    "    links = []\n",
    "    stopwords = set(STOPWORDS)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for tweet in tweets:\n",
    "        tweet = tweet.lower()\n",
    "        curr_string = tweet.split()\n",
    "        for word in curr_string:\n",
    "            if 'http:' in word or 'https:' in word:\n",
    "                links.append(word)\n",
    "                pass\n",
    "            if word in stopwords:\n",
    "                pass\n",
    "            if word.startswith('#'):\n",
    "                word = word.translate(table)\n",
    "                if word not in hashtags:\n",
    "                    hashtags[word] = 1\n",
    "                else:\n",
    "                    hashtags[word] += 1\n",
    "            word = word.translate(table)\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    full_text = \" \".join(word for word in word_dict)\n",
    "    try:\n",
    "        word_cloud = WordCloud(stopwords=stopwords).generate(full_text)\n",
    "        plt.imshow(word_cloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    except:\n",
    "        print(\"No words for the WordCloud apparently\")\n",
    "    target_file.close()\n",
    "    for word in word_dict.copy():\n",
    "        if word in swords.words('english'):\n",
    "            del word_dict[word]\n",
    "    sort_words = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"---Top words---\")\n",
    "    for i in sort_words[:20]:\n",
    "        print(i[0], i[1])\n",
    "    sorted_hashtags = sorted(hashtags.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"---Top hashtags---\")\n",
    "    for i in sorted_hashtags[:20]:\n",
    "        print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "with open(f'{target}.txt', 'r') as test_file:\n",
    "    tweets = test_file.readlines()\n",
    "    for tweet in tweets:\n",
    "        tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "        \"\"\"\n",
    "        # This code block just tested for certain keywords. You can keep it commented out, or uncomment and edit for specific words.\n",
    "        if 'lol' in tweet:\n",
    "            print(tweet)\n",
    "        \"\"\"\n",
    "        sent_analysis = TextBlob(tweet)\n",
    "        if sent_analysis.sentiment.polarity > 0:\n",
    "            sentiments.append('positive')\n",
    "        elif sent_analysis.sentiment.polarity == 0:\n",
    "            sentiments.append('neutral')\n",
    "        else:\n",
    "            sentiments.append('negative')\n",
    "            #print(tweet)\n",
    "    print(f'---{target}---')\n",
    "    try:\n",
    "        print(f'Positive Sentiment Percentage: {sentiments.count(\"positive\")/len(sentiments)}')\n",
    "        print(f'Negative Sentiment Percentage: {sentiments.count(\"negative\")/len(sentiments)}')\n",
    "        print(f'Neutral Sentiment Percentage: {sentiments.count(\"neutral\")/len(sentiments)}')\n",
    "    except:\n",
    "        print(\"Potential singularity here\")\n",
    "    test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
